{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# FINAL MODEL USED: XGBoost\n\n\n!pip install xgboost\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Load train and test sets\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\n\n\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:26.702681Z","iopub.execute_input":"2025-09-14T08:57:26.703487Z","iopub.status.idle":"2025-09-14T08:57:31.435834Z","shell.execute_reply.started":"2025-09-14T08:57:26.703441Z","shell.execute_reply":"2025-09-14T08:57:31.433898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This notebook uses XGBoost for the final prediction and submission. After comparing three models (Linear Regression, Random Forest, and XGBoost), I selected XGBoost based on lowest RMSE.","metadata":{}},{"cell_type":"code","source":"# Visualize target distribution\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 6))\nsns.histplot(train['SalePrice'], kde=True, color='green')\nplt.title(\"Distribution of House Sale Prices\")\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Count\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:31.438085Z","iopub.execute_input":"2025-09-14T08:57:31.438560Z","iopub.status.idle":"2025-09-14T08:57:32.094092Z","shell.execute_reply.started":"2025-09-14T08:57:31.438520Z","shell.execute_reply":"2025-09-14T08:57:32.092997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Define features\nfeatures = ['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageCars', 'YearBuilt']\n\n# Create the imputer using median strategy\nimputer = SimpleImputer(strategy='median')\n\n# Fit on training data and transform both train and test\nX = imputer.fit_transform(train[features])\nX_test = imputer.transform(test[features])\n\n# Define the target as before\ny = train['SalePrice']\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:32.095083Z","iopub.execute_input":"2025-09-14T08:57:32.095450Z","iopub.status.idle":"2025-09-14T08:57:32.111382Z","shell.execute_reply.started":"2025-09-14T08:57:32.095426Z","shell.execute_reply":"2025-09-14T08:57:32.110330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Show coefficients\nprint(\"Model trained!\\nFeature weights:\")\nfor feature, coef in zip(features, model.coef_):\n    print(f\"{feature}: {coef:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:32.113470Z","iopub.execute_input":"2025-09-14T08:57:32.114250Z","iopub.status.idle":"2025-09-14T08:57:32.140082Z","shell.execute_reply.started":"2025-09-14T08:57:32.114218Z","shell.execute_reply":"2025-09-14T08:57:32.139058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nimport numpy as np\n\n# Split training data for validation\nX_train_split, X_valid, y_train_split, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Linear Regression\nlr_model = LinearRegression()\nlr_model.fit(X_train_split, y_train_split)\nlr_preds = lr_model.predict(X_valid)\nlr_rmse = np.sqrt(mean_squared_error(y_valid, lr_preds))\n\n# Random Forest\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train_split, y_train_split)\nrf_preds = rf_model.predict(X_valid)\nrf_rmse = np.sqrt(mean_squared_error(y_valid, rf_preds))\n\n# XGBoost\nxgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\nxgb_model.fit(X_train_split, y_train_split)\nxgb_preds = xgb_model.predict(X_valid)\nxgb_rmse = np.sqrt(mean_squared_error(y_valid, xgb_preds))\n\n# Comparison Table\nprint(\"\\nüîç Model Comparison (RMSE - Lower is Better)\")\nprint(\"Linear Regression RMSE:\", round(lr_rmse, 2))\nprint(\"Random Forest RMSE:    \", round(rf_rmse, 2))\nprint(\"XGBoost RMSE:          \", round(xgb_rmse, 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:32.141307Z","iopub.execute_input":"2025-09-14T08:57:32.141771Z","iopub.status.idle":"2025-09-14T08:57:32.847877Z","shell.execute_reply.started":"2025-09-14T08:57:32.141743Z","shell.execute_reply":"2025-09-14T08:57:32.845623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Models and their RMSE scores\nmodels = ['Linear Regression', 'Random Forest', 'XGBoost']\nrmse_scores = [lr_rmse, rf_rmse, xgb_rmse]\n\n# Create bar chart\nplt.figure(figsize=(8, 5))\nbars = plt.bar(models, rmse_scores, color=['skyblue', 'orange', 'limegreen'])\nplt.title('üîç Model RMSE Comparison')\nplt.ylabel('RMSE (Lower is Better)')\nplt.ylim(0, max(rmse_scores) + 5000)\n\n# Add value labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, height + 200, f'{height:.2f}', ha='center', va='bottom')\n\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:32.849045Z","iopub.execute_input":"2025-09-14T08:57:32.849415Z","iopub.status.idle":"2025-09-14T08:57:33.096311Z","shell.execute_reply.started":"2025-09-14T08:57:32.849374Z","shell.execute_reply":"2025-09-14T08:57:33.095238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.scatter(y_valid, lr_preds, label=\"Linear Regression\", alpha=0.4)\nplt.scatter(y_valid, rf_preds, label=\"Random Forest\", alpha=0.4)\nplt.scatter(y_valid, xgb_preds, label=\"XGBoost\", alpha=0.4)\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)], 'k--', lw=2, label=\"Perfect Prediction\")\n\nplt.xlabel(\"Actual House Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"üìâ Predicted vs Actual House Prices\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:33.097234Z","iopub.execute_input":"2025-09-14T08:57:33.097543Z","iopub.status.idle":"2025-09-14T08:57:33.537856Z","shell.execute_reply.started":"2025-09-14T08:57:33.097521Z","shell.execute_reply":"2025-09-14T08:57:33.536852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare test features and handle missing values\nX_test = test[features].fillna(0)  # Or use the same imputation method you used earlier\n\n# ‚úÖ Predict sale prices using XGBoost (best model)\npredictions = xgb_model.predict(X_test)\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'Id': test['Id'],\n    'SalePrice': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"‚úÖ Submission file created successfully using XGBoost!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:33.538952Z","iopub.execute_input":"2025-09-14T08:57:33.539377Z","iopub.status.idle":"2025-09-14T08:57:33.561426Z","shell.execute_reply.started":"2025-09-14T08:57:33.539354Z","shell.execute_reply":"2025-09-14T08:57:33.559139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show clean table of predictions\nprediction_table = pd.DataFrame({\n    'Id': test['Id'],\n    'Predicted SalePrice': predictions\n})\n\nprint(\"\\nüè° Sample Predictions:\")\nprint(prediction_table.head(20).to_string(index=False))  # Or .tail(), or full list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:33.562756Z","iopub.execute_input":"2025-09-14T08:57:33.563164Z","iopub.status.idle":"2025-09-14T08:57:33.587472Z","shell.execute_reply.started":"2025-09-14T08:57:33.563133Z","shell.execute_reply":"2025-09-14T08:57:33.586450Z"}},"outputs":[],"execution_count":null}]}